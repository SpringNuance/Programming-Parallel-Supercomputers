{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Distributed Quicksort\n",
    "# Introduction\n",
    "In this exercise, you will implement a [stable](https://en.wikipedia.org/wiki/Category:Stable_sorts) version of parallel distributed quicksort across MPI processes to sort data common to all processes. Additionally you will implement it also using GPUs.\n",
    "The exercise consists of three subtasks:\n",
    "\n",
    "  1. Implement the function `quicksort` in `src/quicksort.cu`. Your task here is to implement a  single threaded stable version of quicksort.\n",
    "\n",
    "  2. Extend your implementation to be able to use multiple MPI processes. You need to implement `quicksort_distributed` in `src/quicksort_distributed.cu` (you can and are encouraged to re-use your solution from subtask 1.). Now multiple processes hold a copy of the array and each process should end with the same sorted array. Remember to use multiple MPI tasks  (-n should be larger than 1).\n",
    "\n",
    "  3. Extend your implementation to work on GPUs. Start first by implementing `quicksort` in `src/quicksort_gpu.cu`. Then extend the GPU implementation to multiple GPUs by implementing `quicksort_distributed` in `src/quicksort_distributed_gpu.cu` (you are allowed to and are highly encouraged to re-use your previous solutions for this). In this exercise you should allocate one process per device (for example `--ntasks-per-node=4 --nodes=2 --gres=gpu:teslap100:4`).\n",
    "\n",
    "Note: The performance of your implementation is not graded and to get full points, you only need to ensure your implementations give the correct results. However, you should use the hardware relatively efficiently: an implementation that uses a single CUDA thread in task 3, or a single process in tasks 2 or 3, will receive 0 points. Additionally, try to use CUDA-aware MPI instead of first moving data to the CPU and then communicating in task 3.\n",
    "\n",
    "## Returnables\n",
    "The solutions should be returned in a single zip file named <your student number>.zip, for example 12345.zip. The archive should contain at least `src/quicksort.cu`, `src/quicksort_distributed.cu`, `src/quicksort_gpu.cu` and `src/quicksort_distributed_gpu.cu`. You create the archive with the command zip <your student number>.zip -r src/ (note the -r flag: the archive must contain the src directory).\n",
    "\n",
    "## Getting started\n",
    "Run the following commands to get started:\n",
    "\n",
    "```Bash\n",
    "module load gcc/11.3.0 cmake/3.26.3 openmpi/4.1.5\n",
    "cd pps-example-codes/sheet4\n",
    "mkdir build && cd build\n",
    "cmake .. && make -j\n",
    "cd .. && mkdir yourrundir && cd yourrundir\n",
    "sbatch ../job_scripts/qs_XX.sh\n",
    "```\n",
    "Four binaries should appear corresponding to tasks 1, 2, and 3: `quicksort-serial`, `quicksort-distributed`, `quicksort-gpu` and `quicksort-distributed-gpu`.\n",
    "> See [Triton user guide](https://scicomp.aalto.fi/triton/tut/gpu/) for information on how to queue a batch job. There are also some batch scripts in `sheet4/job-scripts` to get you started.\n",
    "\n",
    "## Hints\n",
    "\n",
    "Only the functions `quicksort` and `quicksort_distributed` are used for grading. You can add additional helper functions as needed.\n",
    "\n",
    "If your implementation is very slow, recall how GPUs differ from CPUs (lecture slides). Do not try to create a for loop inside the kernel that loops over all n elements. In addition, you should strive to avoid branch divergence and ensure that the workload is distributed evenly across the stream processors (CUDA cores).\n",
    "\n",
    "For doing a stable quicksort the prefix sum algorithm is helpful (both when doing it in serial and in parallel). For an introduction to parallel prescan and hints on optimization techniques see [NVIDIAâ€™s article](https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
